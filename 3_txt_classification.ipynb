{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To-do:\n",
    "1. Текст был очищен только от одного мусорного элемента в качестве примера. Исслудйте данные через ноутбук или чере веб-интерфейс BigQuery на предмет других мусорных элементов в тексте, которые не несут в себе никакого особого смысла, а только создают шум в данных. Доработайте функцию очистки тектосвых данных, чтобы в нее можно было передать список ненужного мусора и разом выполнялась очистка\n",
    "2. Проведите стратифицировнную кросс-валидуцию нейросетевого классификатора https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html\n",
    "3. Поэксперементируйте с гиперпараметрами нейросетевого классификатора, постарайтесь повысить качество его работы\n",
    "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "4. Попробуйте использовать не Word2Vec для получения векторого представления текста, а TF-IDF преобразование http://zabaykin.ru/?p=558 http://nlpx.net/archives/57\n",
    "5. Попробуйте использовать более тонко настриваемые алгоритмы нейросетей, например из этого видео https://www.youtube.com/watch?v=cPkH1k3U1c8 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from google.oauth2 import service_account\n",
    "import pandas_gbq \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math as mt\n",
    "import datetime as dt\n",
    "\n",
    "from langdetect import detect\n",
    "import re\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import roc_curve,auc\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CREDENTIALS = service_account.Credentials.from_service_account_info({\n",
    "  \"type\": \"service_account\",\n",
    "  \"project_id\": \"findcsystem\",\n",
    "  \"private_key_id\": \"36721f70db39fbb81267c346c2668f10ff0cf5db\",\n",
    "  \"private_key\": \"-----BEGIN PRIVATE KEY-----\\nMIIEvAIBADANBgkqhkiG9w0BAQEFAASCBKYwggSiAgEAAoIBAQDrna55UYM+/6kh\\nGJnQ28+QWLv2Uv9D7jpSLyENeBFJMWAAWh1ZAlGctc/VgLr1vp5CU/w+2jONawMk\\nFqQFoxcAAUIJT6KHHZqLU8zh7mDccpwUlfn7ZaQUbiR0J6ZH0nHmfQtk1FOs1TZU\\nzJGtyNzUnpePsYJOdpsiZ+Ofh7i3kVubkOoIf/EdNdECOx2bbv7uKkERjIMYCJyh\\nCRXgq2c/19CODEETmvEpUKjYCn9LSspBjTHiUUbT1JQfIAr73r1pXl52dATHIay9\\nf4hNXAr8HUy0DpbiUCUdvzF5N+a8cl5tIHewNfXdv0pWMUmCJetKWk4FKIkMOlDQ\\ny7Of3XdjAgMBAAECggEAGtUcww0w5LJJ6Q57qej2hOpOEZnXHz3Yn3ZljbqgQVUh\\nqTiaAgJ9F9suvZJI+GaKcGRPJDtGRrMqquf3KvstSd9azWOnzzpkWLyk5w/2MPTO\\naJvB2crz+i2m8iF30tMZDJYt/4AcvWCUrOiVKJTo7T/YY9Fj1Rq1xR/OsOLMev8X\\nXbzU+ofW7G8gG/1FFb/qI1FsF0rz7O3zJD79OjJ8jZcz5V4FzCwg8yFumPbhDzlK\\ne2oou3k55Owo2GKdRZUgmlpUSL8s3uk65q9vOOngXwp7qQMQwWueUfx608aIgeT9\\n7OXBuMQ8mXbUkU7lYAWippvwejKQU0BQWpjttisanQKBgQD4kgM+BvV9R5froojE\\niBaXfRf82bJpP9AH0tRWk86kaFhyGR3boioyVENkbMw54ZJVVA4zKukslzrasFUf\\n+bM2595+0mtqMRWw+tiApm3ONwevOPxwn/ijTPJiqKQ96/24aA93oZcfZ9X38/4l\\niFTYvMAG5YjHIsCE7zXJDRHg3QKBgQDyqIukCTIurPyHZ4EOUUZDWY50JTm0mCKg\\nbTNOvM+h9lvnKHYrDrFG9gkb7lbzCZCcJJybtEUEki2D1kuJ4Wqu1/kI2ZPkp5Ie\\n/ViJcxCzCj0fReo2+yOIsHHQ0AQADzmkHrMZYxYWUh8agPOB28Tw/8SZ9Dc1oVrN\\nceTu+kkVPwKBgDgt6AWw1PMHp1JeXcLtbw21/CHtoeEfxwi9obgfl+iYnMTM4G6v\\nbBIL8V9VJ6M9VDFs2fi+jgzB8U9T4yli6hpStXq8XAKYLWrehugstUySK25y1rst\\nrKhbz7x0mQpVt/Zhrn0/TESQ108/GgWplmOV5WCpqAw50oE4/1L9XTkRAoGAD0Ir\\n7beqUScNhhIrGlRf/7Is8/63PzTl0IKtXEEhKUUNiF6R96kn2pd0AS6ehw/N6ROg\\nSWYvhNcQR579BwGGrNHl1fmghBtJY+t4WsRCg4+cQlAqJyTpmhnGPmQmLD7I2Boa\\nFvmVFPg6/nanWT4Rhzn+CdRCeHvZ8ts7kw9n8w8CgYA3NBW4vaWh0ggxbI9dbG06\\nZ6cY+b6guDlky/2HKXgZrCj+6DFVr/8XSdPw7s8ICrbywCbEZVXW+HOFb4fkZr5r\\ni3VO9ihyCf9PzwgQ0hviL0fyJH1Ao3myySh5SyCxgcBoXEJhXG43aY70tQPTNLdP\\n4SiylfHm6gEBJudzYov/iw==\\n-----END PRIVATE KEY-----\\n\",\n",
    "  \"client_email\": \"xsolla-ss-2020-students@findcsystem.iam.gserviceaccount.com\",\n",
    "  \"client_id\": \"105043181870281932141\",\n",
    "  \"auth_uri\": \"https://accounts.google.com/o/oauth2/auth\",\n",
    "  \"token_uri\": \"https://oauth2.googleapis.com/token\",\n",
    "  \"auth_provider_x509_cert_url\": \"https://www.googleapis.com/oauth2/v1/certs\",\n",
    "  \"client_x509_cert_url\": \"https://www.googleapis.com/robot/v1/metadata/x509/xsolla-ss-2020-students%40findcsystem.iam.gserviceaccount.com\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#funtion for getting fresh data from DWH for workload model\n",
    "\"\"\"[summary]\n",
    "Funtion for getting fresh data from BigQuery for workload scoring model\n",
    "[description]\n",
    "Credentials - google service account object with credentials data for project\n",
    "[example]\n",
    "Input: Credentials = credentials_object\n",
    "Output: description\t                                        channel\t category\tcategory_flag\n",
    "        \\nChat transcript:\\nVisitor: I want to buy wit...\tchat\t ps\t        1\n",
    "        \\nChat transcript:\\nVisitor: hell i had a prob...\tchat\t ps\t        1\n",
    "        \\nChat transcript:\\nVisitor: لا استطيع الشراء ...\t chat\t  ps\t     1\n",
    "\"\"\"\n",
    "def getDwhData(Credentials):\n",
    "    statement_bigquery_sql = \" \".join([\"select description, channel, case\",\n",
    "                                       \"when manual_category in ('payment_problem','how_to_pay','howtopay','how_to_play','paystation_error','ps_problem','ps_declined') then 'ps'\",\n",
    "                                       \"else 'other'\",\n",
    "                                       \"end as category,\",\n",
    "                                       \"case\",\n",
    "                                       \"when manual_category in ('payment_problem','how_to_pay','howtopay','how_to_play','paystation_error','ps_problem','ps_declined') then 0\",\n",
    "                                       \"else 1\",\n",
    "                                       \"end as category_flag\",\n",
    "                                       \"from `xsolla_summer_school.customer_support`\",\n",
    "                                       \"where manual_category is not null and\",\n",
    "                                       \"manual_category <> '' and\",\n",
    "                                       \"description is not null and\",\n",
    "                                       \"description <> '' and\",\n",
    "                                       \"channel is not null and\",\n",
    "                                       \"channel <> '' and\",\n",
    "                                       \"channel in ('chat','facebook')\"])\n",
    "    \n",
    "    dataframe_bigquery = pandas_gbq.read_gbq(statement_bigquery_sql,project_id='findcsystem', credentials=Credentials, dialect='standard')\n",
    "\n",
    "    return dataframe_bigquery\n",
    "\n",
    "\n",
    "\"\"\"[summary]\n",
    "Function for transform text to lower case\n",
    "[description]\n",
    "Corpus - list or array object, with text data\n",
    "[example]\n",
    "Input: Corpus = [\"Text_1\",\"Text_2\"]\n",
    "Output: [\"text_1\",\"text_2\"]\n",
    "\"\"\"\n",
    "def lowerCase(Corpus):\n",
    "    corpus = [i.lower().replace('\\n','') for i in Corpus]\n",
    "    return corpus\n",
    "\n",
    "\n",
    "\"\"\"[summary]\n",
    "Function for getting language of text\n",
    "[description]\n",
    "Corpus - list or array object, with text data\n",
    "[example]\n",
    "Input: Corpus = [\"Text_1\",\"Text_2\"]\n",
    "Output: [\"en\",\"ru\"]\n",
    "\"\"\"\n",
    "def getTextLanguage(Corpus):\n",
    "    txt_lang = []\n",
    "    for txt in Corpus:\n",
    "        try:\n",
    "            lang = detect(txt)\n",
    "            txt_lang.append(lang)\n",
    "        except:\n",
    "            lang = 'error'\n",
    "            txt_lang.append(lang)\n",
    "    \n",
    "    return txt_lang\n",
    "\n",
    "\n",
    "\"\"\"[summary]\n",
    "Function for tokenization text\n",
    "[description]\n",
    "Corpus - list or array object, with text data\n",
    "[example]\n",
    "Input: Corpus = [\"word1 word2\",\"word3 word4\"]\n",
    "Output: [[\"word1\",\"word2\"],[\"word3\",\"word4\"]]\n",
    "\"\"\"  \n",
    "def textToTokens(Corpus):\n",
    "    corpus = [i.split() for i in Corpus]\n",
    "    return corpus \n",
    "\n",
    "\n",
    "\"\"\"[summary]\n",
    "Function for clear text after garbage\n",
    "[description]\n",
    "Corpus - list or array object, with text data\n",
    "Substr - string, regular expression\n",
    "[example]\n",
    "Input: Corpus = [[\"word1\",\"word2\"],[\"word3\",\"word4\"]]\n",
    "       Substr = r'word1\n",
    "Output: [[\"word2\"],[\"word3\",\"word4\"]]\n",
    "\"\"\"  \n",
    "def clearTextAfterGarbage(Corpus,Substr):\n",
    "    clear_corpus = []\n",
    "    for text in Corpus:\n",
    "        indexes = []\n",
    "        text_len = len(text)\n",
    "        try:\n",
    "            for i in range(0,text_len):\n",
    "                res = re.search(Substr,text[i])\n",
    "                if res != None:\n",
    "                    indexes.append(i)\n",
    "                \n",
    "            #delete garbage word from text\n",
    "            for index in indexes:\n",
    "                del text[index]\n",
    "        \n",
    "            clear_corpus.append(text)\n",
    "        except:\n",
    "            clear_corpus.append(\"error\")\n",
    "        \n",
    "    return clear_corpus\n",
    "\n",
    "\n",
    "\"\"\"[summary]\n",
    "Build word vector by using pre-trained Word2Vec model\n",
    "[description]\n",
    "Size - lenght of vector\n",
    "Word2Vec_Model - gensim object\n",
    "\"\"\"  \n",
    "def buildWordVector(Text,Size,Word2Vec_Model):\n",
    "    vec = np.zeros(Size).reshape((1,Size))\n",
    "    count = 0.\n",
    "\n",
    "    for word in Text:\n",
    "        try:\n",
    "            vec += Word2Vec_Model[word].reshape((1,Size))\n",
    "            count += 1.\n",
    "        except KeyError:\n",
    "            continue\n",
    "    \n",
    "    if count != 0:\n",
    "        vec /= count\n",
    "    \n",
    "    return vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RAWDATA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting data from dwh\n",
    "SupportRawDataframe = getDwhData(CREDENTIALS)\n",
    "SupportRawDataframe.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SupportRawDataframe.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SupportRawDataframe.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATA PREPROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform text to lower case\n",
    "corpus = SupportRawDataframe.description\n",
    "corpus.astype('str')\n",
    "\n",
    "corpus = lowerCase(corpus)\n",
    "\n",
    "#getting language for text corpus\n",
    "corpus_lang = getTextLanguage(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new dataframe with texts in lower case, without /n symbol and with lang for text\n",
    "SupportRawDataframe['description'] = corpus\n",
    "SupportRawDataframe['lang'] = corpus_lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SupportRawDataframe.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SupportRawDataframe.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting only en texts\n",
    "SupportDataframe_eng = SupportRawDataframe[SupportRawDataframe.lang == 'en'][:]\n",
    "SupportDataframe_eng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text tekenization\n",
    "tokenization = textToTokens(SupportDataframe_eng.description)\n",
    "SupportDataframe_eng['description'] = tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SupportDataframe_eng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = list(SupportDataframe_eng.description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning text after garbage\n",
    "tests_clear = clearTextAfterGarbage(texts,r'chat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SupportDataframe_eng['description'] = tests_clear\n",
    "SupportDataframe_eng"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRAIN TEST SPLIT DATAFRAMES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of unique categories\n",
    "unique_categories = np.unique(SupportDataframe_eng.category)\n",
    "unique_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptions = SupportDataframe_eng['description']\n",
    "categories = SupportDataframe_eng['category_flag']\n",
    "XTrain,XTest,YTrain,YTest = train_test_split(descriptions,\n",
    "                                             categories,\n",
    "                                             stratify = categories,\n",
    "                                             test_size = 0.2,\n",
    "                                             random_state = 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FROM TEXTS TO VECTORS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize Word2Vec model for embedding words to vectors\n",
    "NDim = 100\n",
    "Imdb_w2v = Word2Vec(size = NDim,min_count = 10)\n",
    "Imdb_w2v.build_vocab(XTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Imdb_w2v.train(XTrain,total_examples = Imdb_w2v.corpus_count,epochs = Imdb_w2v.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embedding training messages to vectors for neutral classifier\n",
    "TrainVecs = np.concatenate([buildWordVector(i,NDim,Imdb_w2v) for i in XTrain])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Imdb_w2v.train(XTest, total_examples = Imdb_w2v.corpus_count, epochs = Imdb_w2v.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TestVecs = np.concatenate([buildWordVector(i,NDim,Imdb_w2v) for i in XTest])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TestVecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CROSSVALIDATION AND BUILD CLASSIFIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TextClassifier = MLPClassifier(hidden_layer_sizes = (20,10), max_iter = 1000, random_state = 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Scores = cross_val_score(TextClassifier, TrainVecs, YTrain, cv = 5)\n",
    "print(Scores)\n",
    "print(np.mean(Scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TextClassifier.fit(TrainVecs,YTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = TextClassifier.predict(TestVecs)\n",
    "print(confusion_matrix(YTest,pred))\n",
    "print(classification_report(YTest,pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
